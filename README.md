# üìä Vendor Performance Data Analytics Project

End‚Äëto‚Äëend, reproducible pipeline to ingest raw CSVs, build a consolidated **Vendor Sales Summary**, and analyze vendor performance using Python, SQL, and notebooks.

---

## üî• Highlights

* **Database ingestion** from `dataset/*.csv` into MySQL with SQLAlchemy.
* **Consolidated summary table** (CTE-based SQL joining purchases, sales, and freight).
* **Feature engineering**: Gross Profit, Profit Margin, Stock Turnover, Sales‚Äëto‚ÄëPurchase Ratio.
* **Exploratory analysis** in Jupyter (Pareto charts, distributions, confidence intervals, top N vendors/brands).
* **Logging** to `logs/` for ingestion & summary creation.
* **GitHub‚Äëfriendly** structure with guidance for large datasets.

---

##  Dataset

The full dataset exceeds GitHub‚Äôs size limit, so it‚Äôs hosted externally for easy access.

**Download the complete dataset here (Google Drive):**  
[Full Dataset (Google Drive)](https://drive.google.com/file/d/1jZ90pS2vJhti0wJ-I0XK9_4zatZ_gbED/view?usp=sharing)

> ‚Ñπ Tip: After opening the link, click the "Download" icon at the top right to obtain the files.

A small sample dataset is included in this repository for quick testing and demo:
- `dataset/sample/sample_purchases.csv`
- `dataset/sample/sample_sales.csv`

You can run the notebooks/scripts using these smaller files, or replace them with the full dataset once downloaded.


## ‚úÖ Prerequisites

* **Python** 3.10+
* **MySQL** 8.0+ (for the MySQL pipeline). SQLite can be used for quick local experiments if you adapt connection code.
* **pip/venv** for dependency management

---


## üîê Configure Environment Variables

Create a real `.env` file from the template below:

**`.env.example`**

```
# MySQL connection
MYSQL_USER=your_user
MYSQL_PASSWORD=your_password
MYSQL_HOST=localhost
MYSQL_PORT=3306
MYSQL_DATABASE=inventory_db
```

> Never commit your real `.env` to GitHub. `.gitignore` should exclude it.

**Example `.gitignore` entries**

```
# virtual envs
.venv/

# OS/editor junk
.DS_Store
.ipynb_checkpoints/
__pycache__/

# data
*.db
*.zip
*.gz
*.parquet
*.feather
*.sqlite
*.sqlite3

dataset/*
!dataset/sample/

# secrets
.env
logs/
```

---

## üì• Step 1 ‚Äî Prepare the Data

Place your raw CSVs inside `dataset/` with the expected names:

* `purchases.csv`
* `sales.csv`
* `vendor_invoice.csv`
* `purchase_prices.csv`

> If you have different filenames/paths, that‚Äôs fine‚Äîjust keep them inside `dataset/` (the ingestion script picks up all `*.csv`) or adjust the code accordingly.

For GitHub, include **tiny samples** in `dataset/sample/` and share links to full datasets in the README.

---

## ‚öôÔ∏è Step 2 ‚Äî Ingest to MySQL

The ingestion script loads **every CSV file** in `dataset/` into MySQL, naming each table by the CSV filename (without extension).

**Run:**

```bash
# from repo root
python scripts/ingestion_db.py
```

**What it does:**

* Reads `dataset/*.csv`
* Creates/overwrites MySQL tables named after each CSV
* Logs progress to `logs/ingestion_db.log`

**Common issues & fixes**

* *Connection error*: verify `.env` values and that MySQL is running.
* *No CSVs found*: ensure files are in `dataset/` (not `datasets/`) and end with `.csv`.
* *Datatype quirks*: Pandas will infer types‚Äîoverride dtypes in `read_csv` if needed.

> üí° Tip: For very large CSVs, consider chunked ingestion (e.g., `chunksize=100_000`) and add `if_exists='append'` in `to_sql` to stream rows.

---

## üßÆ Step 3 ‚Äî Build the Vendor Sales Summary

The summary pipeline:

1. Uses a **CTE SQL** query to combine purchases, sales, and freight by vendor/brand.
2. Cleans and engineers features.
3. Writes the final DataFrame back to the database (table: `vendor_sales_summary`).

**Run:**

```bash
python scripts/get_vendor_summary.py
```

**What it computes**

* **GrossProfit** = `TotalSalesDollars ‚àí TotalPurchaseDollars`
* **ProfitMargin** (%) = `GrossProfit / TotalSalesDollars * 100`
* **StockTurnover** = `TotalSalesQuantity / TotalPurchaseQuantity`
* **SalesToPurchaseRatio** = `TotalSalesDollars / TotalPurchaseDollars`

**Outputs**

* MySQL table: `vendor_sales_summary`
* Log file: `logs/get_vendor_summary.log`

**Troubleshooting**

* *‚Äúno such table ‚Ä¶‚Äù*: ensure ingestion created `purchases`, `sales`, `vendor_invoice`, `purchase_prices` tables.
* *Nulls/NaNs*: the cleaning step fills missing values with zeros. Adjust if business logic requires otherwise.

---

## üß™ Step 4 ‚Äî Explore in Notebooks

Open Jupyter and run analyses end‚Äëto‚Äëend, or use the provided notebooks:

```bash
jupyter notebook
```

* **DataBaseIngestion.ipynb** ‚Äî Validate ingestion, preview tables
* **Exploratory\_Data\_Analytics.ipynb** ‚Äî EDA, top vendors/brands, distributions
* **vendor\_performance\_analysis.ipynb** ‚Äî KPIs, Pareto charts, confidence intervals

### Example: Pareto Chart for Vendor Contribution

```python
fig, ax1 = plt.subplots(figsize=(12, 6))
sns.barplot(
    x=top_vendors['VendorName'],
    y=top_vendors['PurchaseContribution (%)'],
    palette='mako', ax=ax1
)
for i, v in enumerate(top_vendors['PurchaseContribution (%)']):
    ax1.text(i, v + 1, f"{v}%", ha='center', fontsize=9)
ax2 = ax1.twinx()
ax2.plot(
    top_vendors['VendorName'],
    top_vendors['Cumulative_Contribution (%)'],
    color='red', linestyle='dashed', marker='o', label='Cumulative %'
)
ax1.set_xticklabels(top_vendors['VendorName'], rotation=90)
ax1.set_ylabel('Purchase Contribution (%)')
ax2.set_ylabel('Cumulative Contribution (%)')
ax1.set_title('Pareto: Vendor Contribution to Total Purchases')
ax2.axhline(80, color='gray', linestyle='dotted', alpha=0.6)  # 80/20 reference
ax2.legend(loc='upper right')
plt.tight_layout(); plt.show()
```

### Example: Confidence Interval Utility

```python
from scipy import stats
import numpy as np

def confidence_interval(series, confidence=0.95):
    series = pd.to_numeric(series, errors='coerce').dropna()
    mean_val = series.mean()
    std_err = series.std(ddof=1) / np.sqrt(len(series))
    t_crit = stats.t.ppf((1 + confidence)/2, df=len(series) - 1)
    moe = t_crit * std_err
    return mean_val, mean_val - moe, mean_val + moe
```

---

## üõ†Ô∏è How to Adapt / Extend

* **Rename columns**: If your CSVs use different headers, update the SQL CTE and cleaning logic.
* **Add KPIs**: Extend the feature engineering section to compute lead time, fill rate, returns, etc.
* **Switch databases**: Replace the SQLAlchemy engine with SQLite/Postgres as needed.
* **Automate**: Wrap the pipeline in a `main.py` or Makefile; schedule via cron/Airflow.

---

## üß∑ Reproducibility & Data Policy

* Commit only code, notebooks, and tiny **sample** datasets.
* Host full data externally and link it in the README.
* Use `.env` for secrets; never hard‚Äëcode credentials.

---

## üßπ Housekeeping

* **Logs**: Inspect `logs/ingestion_db.log` and `logs/get_vendor_summary.log` when debugging.
* **Code style**: Optional‚Äîadd `ruff`/`flake8` and `black` to pre‚Äëcommit hooks.
* **Testing**: Optional‚Äîadd unit tests for cleaning and KPI formulas.

---

## üöÄ Quickstart (TL;DR)

1. `git clone ‚Ä¶ && cd vendor-performance-analysis`
2. `python -m venv .venv && .venv/Scripts/activate`
3. Create `.env` with MySQL creds
4. Put CSVs in `dataset/` (or use `dataset/sample/`)
5. `pip install -r requirements.txt`
6. `python scripts/ingestion_db.py`
7. `python scripts/get_vendor_summary.py`
8. `jupyter notebook` ‚Üí open and run the analysis notebooks

---

## ‚ùìFAQ / Troubleshooting

**Q: Git push rejected due to large files**
A: Add big files to `.gitignore`, keep samples only, and host full data externally (Drive/Kaggle). If you must, use Git LFS (paid tiers for multi‚ÄëGB).

**Q: `no such table` when building summary**
A: Run the ingestion step first to create the base tables in the database.

**Q: Wrong column names / `KeyError`**
A: Print `df.columns` and align the code (SQL & cleaning) to your schema.

**Q: MySQL auth errors**
A: Verify `.env` values and that the MySQL instance is reachable (port/user/password). Grant privileges if needed.

---

## üìú License

Choose a license (e.g., MIT) and add it as `LICENSE` in the repo root.

---

## üôå Acknowledgements

* Python, Pandas, SQLAlchemy, MySQL, Matplotlib, Seaborn
* Everyone who maintains open‚Äësource tools used in this project
